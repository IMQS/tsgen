.67 Implement Connection and Channel as methods of TSQueue and not for the 
	consumer and publisher individually
.66 Simplify publisher
.65 Update the initialisation of the RABBIT form in Output
.64 Update Output to be more elegant in handling the RABBIT form and adapt
	the assignment of Jobs (atomise)
.63 Simplify consumer
.62 Add properties Enable and Ack for RABBIT queue

.61 Implement the Output publishing for queues
.60 Implement the Output initialisation for the RABBIT form
.59 Subscribe queues to either PUBLISH, CONSUME or BOTH
.58 Add configuration items to instantiate queues
.57 Reorganise the items in TSProperties according to the classification in the 
	documentation on tsgen at https://github.com/IMQS/tsgen.git.  Note that changes
	made to the README.md file has been lost at some point, either to not committing
	them or an overwrite during the CITUS push and/or Report pull request.
.56 Call on the Rabbit Init function that currently kicks all of the RabbitMQ
	consumer listeners into life
.55 Add a slice of Rabbit to the TSOutput structure
.54 Implement consumers and publishers of RabbitMQ
.53 Implement the necessary structures to support RabbitMQ as an Output format
.52 Add package rabbit for RabbitMQ Form via rabbit.go, consume.go and publish.go 

.51 For OpenTSDB, at a batch value of 1, the JSON looks a bit different
	than for more than one sample at a time.  Implement code to handle this
	scenario.  TODO: Known issue with the marshalling that does not 
	create the correct JSON structure for a single value yet, although the 
	framework to handle this scenario has been implemented.
.50 Insert a print out of the sample write rate achieved
.49 Update code to actually write to 50 000 sites and not the same seed random
	values that only wrote to a much smaller number of sites.

.48 No need to update code to perform benchmarks to different data base
	types that are currently supported as compile time.
.47 Implement the Format function in out.go to be more resiliant
	against possible data infringements in the concurrent architecture
	for the workers or Spools on the HTTP output.  TODO:  There remains
	some effort to ensure that the data for each process is completely 
	isolatd from the next process.  Data still seem to be lost when 
	written to the same metric from more than a single Spool.
.46 Rename the Format config item to Form throughout
.45 Update the TSDestination structure to contain a pointer to the 
	TSProperties structure that is parsed from config.JSON instead of duplicating each field that is required in both structures
.44	Kairos and OpenTS data timeseries databases now use the same code 
	base with the marshalling interfaces being the only real difference
.43 Rewrite REST api section to enable the use of the same data point
	structures. 

.42  Update openTSDB to handle multiple sites

.41 Current configuration for opentsdb benchmarking.  Going to commit with 
	this configuration already set up since current benchmarking is taking place on openTSDB.

.40 Current status of code been used to interface to Newts, KairosDB and 
	OpenTSDB.
.39 Som eminor updates to allow seamless switching between the different 
	database interfaces 

.38 Edit the opentsdb.go file and function structure to fit the approach
	identified and used by newts and kairosdb in an effort to standardise the 
	interfaces as far as possible. One of the outcomes is to use a common
	structure to describe a data point before it is marshalled to the 
	respective databases.
.37 Add the opentsdb.go file to the rest package to incorporate the interface
	to the OpenTSDB database for benchmarking

.36 Update the code in out.go that creates and sends the HTTP request to use
	the updated HTTP request structures
.35 Update the kairosdb marshalling to utilise the same methodology as newts.
	This ensures that movement between database interfaces are seamless
.34 Add the newts.go file that contains the code that marshalls time series
	data into the required format for the newts database

.33 To allow for the 'Spools' functionality, the REST property of the output
	had to be extended to an array, to allow independent formatting of each
	SpoolStamp and SpoolValue set.
.32 Create the SpoolStamp and SpoolValue sets in the TSDestination structure
.31 Output time stamp and value arrays were extended to multidimensional arrays
	to maintain index independance between the concurrent HTTP spools (within
	a data Create process)
.30 RANDOM type may also be used with a compound type as one of the signal
	types defined in the array.  NOTE that frequency should also be populated
	as is required for compound signals, although it may not be used in the 
	signal generation
.29 RANDOM data type added to the data transforms.  Random values are biased
	around 'Bias' property.  Absolute values between 0 and 'Amp'. Random 
	generator to determin the sign of the value which is multiplied to the 
	0 to 'Amp' range absolute value.
.28 env.bat file created to set up environment before running code or 
	executable
.27 Each database interface is to be defined in package rest with the same 
	function names implementing similar functions on each database interface
.26 HTTP data is now processed by combining calls to the Create REST function
	instead of calling the Batch function
.25 If the'Now' flag is set, the Start time is replaced with the current time
	at the start of creating the data series.  
.24 HTTP data released from output may now concurrently be processed from 
	the specified number of 'Spools', each 'Batch' being awarded to the first
	Spool that is available to handle another job
.23 Data at present can be 'distributed' between any amount of 'sites'. This
	updates the name of the data set with an appended site number.
.22 User specified 'Batch' page sizes that fall back on default values if not 
	specified
.21 Add 'Batch', 'Sites, 'Spools', 'Distribute' and 'Now' properties. 

.20	Move most of the const declaration for 'enum' types to the config.go file
	and redefine the types in the TSProperties struct to make use of these
	types.
.19 Implement the REAL Mode to release HTTP requests according to the relative
	differences between absolute points in time generated to fit within the
	Duration (in seconds) defined in the config file
.18 Update the Type array to take capitalised strings as is the way most other
	config settings are defined in the config file
.17 Add Mode to the configuration file.  Mode determines the way in which
	the data set is released through the Output.

.16 Extend `Bias`, `Type`, `Freq` and `Amp` to arrays to support the compound
.15	Add Bias configuration item
.14 Implement a compound type signal that is built out of an array of Sin and 
	Cos types at this point in time

.13 Implement the HTTP POST support for single as well as batch HTTP requests
.12 Implement 'High' and 'Low' scaling factors for the Logic data type to
	scale the HIGH and LOW logical signal values
.11 Use the events and state to generate Logic signals that 
	change at random intervals throughout the time series distribution
	but only the amount of times specified
.10 Implement configuration properties to allow for the Logic data type
.9 	Implement a toggle switcher that flips between STATEs on each toggle event
.8 	Register toggle events on predetermined amount of samples
.7	Generate event nodes
.6	Implement quick sort utility since sort package does not support sorting 
	[]UInt64

.5	Add the basic moment measurement tool to the profiling package, use it to 
	measure the time it takes to generate an output
.4	Remove the verbose display of each data point generated.  Send to CSV
	output if data has to be evaluated or visualised.
.3. Add events to time series generation and return as structure that 
	contains relevant event information for exact point in series, this commit 
	DOESN'T make use of the event YET.	
.2. More elaborate comments in the packages.
.1.	Change package 'file' to 'out' since the possible destinations have been 
	updated to include queues, http requests etc.
